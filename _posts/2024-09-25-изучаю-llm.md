---
date: 2024-09-25 14:20:14 +0000
excerpt: 'Пост посвящен обзору материалов для изучения больших языковых моделей (LLM):
  рассматриваются книги, научные обзоры, видео и блоги, которые помогают разобраться
  в архитектуре, обучении и применении LLM. Материалы охватывают широкий спектр тем
  - от технических деталей и инфраструктуры до практических аспектов использования
  языковых моделей в различных областях.'
layout: post
source_type: telegraph
source_url: https://telegra.ph/Izuchayu-LLM-09-25
tags:
- machinelearning
- жека
- llm
telegram_url: https://t.me/varim_ml/134
telegraph_url: https://telegra.ph/Izuchayu-LLM-09-25
title: Изучаю LLM
views: 4848
---

# Изучаю LLM  


[Evgenii Nikitin](https://t.me/crazyfrogspb)  


Раньше я следил за миром LLM больше из любопытства - почитывал ключевые и заинтересовавшие статейки, игрался со свежими модельками, читал срачи в Твиттере. Сейчас по работе понадобилось серьёзнее углубиться в эту пропасть, поэтому я как настоящий 32-летний дед решил взяться за книжки и статьи. Сегодня расскажу про материалы, которые изучил, для чего они хороши, а для чего - не очень. Книжки можно купить на Manning, скачать на каких-нибудь твирпиксах или на худой конец написать мне в личку =)

### Книги

#### [LLMs in Production](https://www.manning.com/books/llms-in-production)

![](/assets/images/c2a34823.png)Большое внимание уделяется инфраструктурному компоненту инференса

Для меня это была самая полезная книжка из всех прочитанных. Она не даёт какой-то безумной глубины, зато проводит по всем основным аспектам обучения, файнтюнинга и инференса LLM. После прочтения сложится общее впечатление о более-менее актуальном состоянии самых разных тем (в случайном порядке) - токенизация, квантизация, параллелизация, векторные БД, текстовые метрики, промптинг, PEFT, инференс, мониторинг, RAG, автоскейлинг, фронтенд для LLM. В общем, очень хорошо подходит для старта в мире LLM или в качестве рефрешера.

#### [Building LLMs for Production](https://www.oreilly.com/library/view/building-llms-for/9798324731472/)

![](/assets/images/8d151245.png)Пример пайплайна, который разбирается в книжке

Книга со значительно большим количеством кода, в основном она посвящена созданию разных ИИ-систем на основе LLM. На примере библиотек Langchain и LlamaIndex разбирается как:

  * создавать сложные промпты и их цепочки
  * получать ответ от LLM в нужном формате
  * использовать сторонние инструменты типа поиска
  * чанковать и индексировать документы, прикручивать RAG и улучшать качество query для получения более релевантных документов
  * создавать сложных агентов, которые сами могут вызывать разные тулы, "размышлять" и совершать какие-то действия
  * файнтюнить модели с помощью LoRA и QLoRA и заводить свой RLHF



Короче, темы разбираются плюс-минус похожие (правда, инференс и деплой раскрываются намного меньше), но упор именно на код, а сами концепты, на мой взгляд, раскрываются хуже.

#### [Build a LLM from Scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

![](/assets/images/89ded1a0.png)

Книга от моего любимого Себастьяна Рашки. Она совсем про другое - разбираем верхнеуровнево, как устроена архитектура языковых моделей, как они обучаются, какие используются лоссы и техники. А главное - пишем всё это на ванильном Пайторче. Если прочитать, а ещё лучше покодить вместе с автором, то точно хорошо разберётесь во всех этих этеншнах, энкодерах, декодерах, маскингах, нормализациях, токенизациях и лоссах. А ещё в книге очень красивые схемы с пояснениями.

### Surveys

#### [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712) и [репозиторий](https://github.com/Mooler0410/LLMsPracticalGuide)

Сама папира не так чтобы очень, но в репозитории лежит много ссылок на знаковые LLM-папиры по разным темам. Она не обновляется, так что свежак можно найти, например, в этой [репе](https://github.com/Hannibal046/Awesome-LLM).

#### [A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2405.06211)

![](/assets/images/6caea6c3.png)

Довольно подробный обзор разных RAG-техник - как простейших, так и более сложных - например, с использованием реранкеров или с поиском новых документов уже по ходу генерации токенов.

А [здесь](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) можно почитать про разные стратегии чанкинга документов для индексации. И [библиотека](https://github.com/NirDiamant/RAG_Techniques) с простыми и продвинутыми способами имплементации RAG.

#### [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219) и [A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models](https://arxiv.org/abs/2401.01313)

Термин "галлюцинации" давно выплыл из мира ML-щиков и знаком всем, кто когда-либо имел дело с LLM. Эти статьи позволяют разобраться с видами галлюцинаций (противоречия с инпутом, противоречия внутри аутпута, фактические ошибки), а также пройтись по методам их минимизации - как с дообучением модели, так и без него (промпты, RAG, тюнинг гиперпараметров инференса).

#### [A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions](https://arxiv.org/abs/2406.03712)

Из всех обзоров медицинских LLM мне больше всего понравилась эта статья, без воды, всё важное упоминается, есть [репозиторий](https://github.com/KaiHe-CatOwner/LLM-for-Healthcare) со всеми ссылками. По медицине ещё могу посоветовать [MEDIC](https://arxiv.org/abs/2409.07314) \- статью про то, как оценивать LLM на медицинских задачах.

#### Ещё

  * [Reasoning with Large Language Models](https://arxiv.org/pdf/2407.11511v1)
  * [Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely](https://arxiv.org/abs/2409.14924)



### Видео

#### [Карпатый](https://karpathy.ai/zero-to-hero.html)

Живая классика от мэтра DL - мини-курс [Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ). Идея простая - если мы закодим всё с нуля - [свой автоград](https://github.com/karpathy/micrograd), [свой GPT](https://github.com/karpathy/nanoGPT/tree/master/), то явно круто разберёмся в том, как это всё работает. Вот видосы непосредственно про LLM:

  * [Let's build GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY)
  * [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
  * [Let's reproduce GPT-2](https://www.youtube.com/watch?v=l8pRSuU81PU&t=828s)



#### [Теоретический туториал: Как учить большие языковые модели](https://www.youtube.com/watch?v=T3dCGPaCu5w)

Неплохой видос на русском с описанием техник и трюков, которые используются при обучении LLM.

#### [LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://www.youtube.com/watch?v=Mn_9W1nCFLo)

Ллама - одна из самых важных архитектур в мире LLM, это не просто одна из самых качественных опенсорс-моделей, но и основа для кучи зафайнтюненных версий, в том числе в медицине. Поэтому вполне недурная идея - понять, как работают компоненты лламы, которые (помимо данных) и отвечают за её качество. В видосе разбирается и интуиция, и немного математика, и код.

### Прочее

#### [Galileo Blog](https://www.rungalileo.io/blog)

Ребята продают свой продукт, но заодно ведут и хороший блог с практическими постами - например, как [выбрать лучший реранкер для RAG](https://www.rungalileo.io/blog/mastering-rag-how-to-select-a-reranking-model) или [выбрать лучший метод чанкования](https://www.rungalileo.io/blog/mastering-rag-advanced-chunking-techniques-for-llm-applications).

#### [Ahead of AI](https://magazine.sebastianraschka.com/)

Блог всё того же Себастьяна Рашки, где он регулярно пишет об LLM со ссылками на последние папиры и их кратким обзором, а также даёт много практических советов по обучению моделек. Например, можно достаточно подробно изучить тему instruction tuning:

  * [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives?utm_source=profile&utm_medium=reader2)
  * [Instruction Pretraining LLMs](https://magazine.sebastianraschka.com/p/instruction-pretraining-llms)
  * [Tips for LLM Pretraining and Evaluating Reward Models](https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms?utm_source=profile&utm_medium=reader2)
  * [How Good Are the Latest Open LLMs? And Is DPO Better than PPO?](https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms?utm_source=profile&utm_medium=reader2)



#### Гайды и мануалы

Конечно же, в интернете полно мануалов по самым разным темам:

  * В документации библиотек полно хороших примеров их использования - например, тот же [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval/)
  * [Replacing my best friends with an LLM trained on 500,000 group chat messages](https://www.izzy.co/blogs/robo-boys.html) \- рабочая инструкция для тех, кто хочет научить LLM пародировать групповые дружеские чаты
  * [The Novice's LLM Training Guide](https://rentry.org/llm-training) \- гайд с полезными базовыми советами по обучению LLM


