---
date: 2024-04-12 13:38:07 +0000
excerpt: Пост содержит обзор полезных материалов и ресурсов для изучения библиотеки
  PyTorch, включая технические статьи, туториалы и объяснения внутренней архитектуры
  фреймворка. В материале рассматриваются ключевые компоненты PyTorch, механизм автоматического
  дифференцирования, ссылки на углубленные технические источники и дополнительные
  инструменты для работы с библиотекой.
layout: post
source_type: telegraph
source_url: https://telegra.ph/PyTorch-i-okolo-nego-04-12
tags:
- жека
telegram_url: https://t.me/varim_ml/122
telegraph_url: https://telegra.ph/PyTorch-i-okolo-nego-04-12
title: Полезные материалы про PyTorch
views: 4185
---

# Полезные материалы про PyTorch  


[Evgenii Nikitin](https://t.me/crazyfrogspb)  


#### Внутренности торча

![](/assets/images/ac928a88.png)

Даже если вы не хотите контрибьютить в Пайторч, хотя бы поверхностное понимание устройства его внутренностей - небесполезная инвестиция времени. Всё-таки это ключевая библиотека многих продакшн DL-систем (у нас - всех).

Помимо [документации ](https://pytorch.org/docs/stable/index.html)и [блога](https://pytorch.org/blog/) (очень рекомендую), в этом плане мне особенно нравятся два материала. Один называется [**PyTorch Internals**](http://blog.ezyang.com/2019/05/pytorch-internals/), он был опубликован в 2019 году, так что в некоторых местах устарел, но основные концепты (тензор, storage, autograd, ATen, kernel) описаны хорошо и с симпатичными слайдами.

В декабре 2023 года появилась вот такая [**презентация**](https://blog.christianperone.com/2023/12/pytorch-2-internals-talk/), к сожалению, видео выступления было утеряно в процесса записи. В начале идёт разбор основных компонентов торча, а вторая часть фокусируется на новинках второй версии - TorchDynamo, Inductor, ExecuTorch.

Если вы только знакомитесь с библиотекой, то могу порекомендовать [старую серию постов](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/) от Paperspace. Много интересного можно почерпнуть в ответах на StackOverflow ([пример про Storage](https://stackoverflow.com/questions/73761229/pytorch-tensor-and-its-transpose-have-different-storage)) и даже в дискуссии с GPT-4.

![](/assets/images/baf9ab8b.png)

#### Autograd

PyTorch по своей сути изначально явлется фреймворком Autograd - обратного автоматического дифференциирования. Материалы для более глубокого понимания автограда:

  * [Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html) \- текстовое описание основных принципов в документации Пайторча
  * [A Gentle Introduction to torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) \- официальный туториал, в котором в том числе упоминается, что torch.autograd вообще-то считает произведение вектора (градиентов) и Якобиана (матрицы производных). Просто обычно мы применяем .backward(gradient) к скалярному лоссу - например, сумме, и в качестве [вектора имплицитно берётся единица](https://discuss.pytorch.org/t/why-do-we-need-to-pass-the-gradient-parameter-to-the-backward-function-in-pytorch/116079/2). В этой [статье](https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29) разбираются разные варианты инпутов и аутпутов и эксплицитное прокидывание градиентов. Ещё объяснения и примеры можно найти [тут](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments) и [тут](https://stackoverflow.com/questions/67472361/using-pytorchs-autograd-efficiently-with-tensors-by-calculating-the-jacobian).

![](/assets/images/f7d831e3.png)

  * Когда-то в университетах мы учили сеточки, реализуя слои с нуля в numpy. Аналогично можно реализовать и свой autograd-движок, как [это сделал Андрей Карпаты](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ). Вот ссылка на весь [плейлист](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ).



#### Прочее

Ещё несколько рандомных около-пайторч материалов, которые мне нравятся:

  * [How Are Convolutions Actually Performed Under the Hood?](https://freedium.cfd/https://towardsdatascience.com/how-are-convolutions-actually-performed-under-the-hood-226523ce7fbf) \- как ускорить наивную имплементацию конволюции почти до уровня Пайторча. Помогает разобраться во внутренностях конволюции.
  * [Tensor Puzzles](https://github.com/srush/Tensor-Puzzles) \- реализуем разные операции на тензорах с помощью бродкаста. Когда устаю работать, пробую решить 1-2 примерчика в перерывах.
  * Pytorch Toolbelt - либа с разными фишками, от TTA и тайл-инференса до ансамблирования моделей.
  * [minitorch](https://minitorch.github.io/) \- учебная библиотека, которая реимплементирует Пайторч на чистом Питоне
  * [PyTorch Performance Tuning Guide](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf) \- преза от NVIDIA 2020 года с советами по оптимизации
  * [Feeding the Beast: The Data Loading Path for DL Training](https://towardsdatascience.com/feeding-the-beast-understanding-and-optimizing-the-data-loading-path-for-training-deep-learning-f896548dca89) \- не про пайторч, но интересная ссылка про оптимизации чтения данных при обучении
  * [Making DL Go Brrrr From First Principles](https://horace.io/brrr_intro.html) \- оптимизация нейронок с точки зрения compute и memory bandwidth, ищем боттлнеки


