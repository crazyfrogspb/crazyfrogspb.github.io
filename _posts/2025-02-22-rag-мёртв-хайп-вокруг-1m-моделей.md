---
date: 2025-02-22 16:04:37 +0000
excerpt: Пост посвящен исследованию возможностей и ограничений больших языковых моделей
  при работе с длинным контекстом, рассматривая различные технические подходы к расширению
  контекстного окна до миллиона токенов. Несмотря на технологический прогресс, RAG
  остается эффективным инструментом для решения практических задач с большими объемами
  информации, где важно цитирование и быстрый поиск релевантных данных.
layout: post
source_type: telegraph
source_url: https://telegra.ph/RAG-myortv-02-11
tags:
- жека
- llm
telegram_url: https://t.me/varim_ml/143
telegraph_url: https://telegra.ph/RAG-myortv-02-11
title: RAG мёртв? Хайп вокруг 1M-моделей
views: 3447
---

# RAG мёртв? Хайп вокруг 1M-моделей  




На Реддите и в твиттере время от времени появляются такие посты

![](/assets/images/8b545971.png)

И такие

![](/assets/images/411b6ad9.png)

У людей, которые уже успели повыкатывать приложения с LLM под капотом в продакшн, немедленно начинает бомбить в комментариях. Почему? Давайте разбираться.

#### Эра большого контекста

Проблему увеличения контекста жаждут решить с самого появления архитектуры трансформера. В отличие от RNN, которые в теории могут проносить контекст сквозь всю последовательность независимо от её длины, трансформеры ограничены тем контекстом, который может поместиться в вычислительную память. Большой документ можно разбить на куски, обрабатывать их отдельно и потом как-то агрегировать результаты, но такой способ подойдёт не для всех задач. В бенчмарке [RULER](https://github.com/NVIDIA/RULER), созданном специально для оценки качества работы LLM на длинном контексте, можно видеть, насколько сильно падает качество работы моделей, которые обучались только на коротком контексте без специальных архитектурных трюков.

Одной из заметных попыток решения проблемы контекста в этеншн-моделях в своё время стала статья [Transformer XL](https://arxiv.org/abs/1901.02860). Её основные идеи:

  * Давайте дробить длинный текст на сегменты, но будем кэшировать hidden state всех предыдущих кусков. Градиент до них доходить не будет, но по крайней мере у сети будет вся информация из предыдущих сегментов текста.
  * Нужно использовать относительный, а не абсолютный positional encoding, иначе модель не будет нормально работать на документах длиннее, чем в трейн-сете.



При таком подходе память рано или поздно всё равно закончится - ведь мы кэшируем hidden state всех предыдущих сегментов. Метод [Infini-Attention](https://arxiv.org/abs/2404.07143), который вроде как использовался в первой версии [Gemini](https://ai.google.dev/gemini-api/docs/long-context) с миллионным контекстом, теоретически может работать с бесконечным контекстом за счёт использования компрессивной памяти фиксированного размера. Идея концептуально напоминает RNN-модели:

![](/assets/images/a38c7387.png)

  * Последовательность делится на сегменты фиксированного размера - скажем, 2048 токенов
  * Вместо хранения всех предыдущих ключей (K) и значений (V), мы используем "память" фиксированного размера, которая модифицируется на основе текущих K и V. Если информация из текущих KV не сильно отличается от уже сохранённой, то память можно особо не обновлять, нам интересна только новая информация
  * При калькуляции этеншна для текущего токена используется и локальный этеншн, и этеншн c памятью. Они агрегируются простым суммированием с весами. По итогам обучения по этим весам можно увидеть, что образуется два типа этеншн-голов - "специализированные", которые берут информацию только из локального контекста или только из памяти, и "миксованные", которые смешивают информацию



Ребята из HF [решили проверить](https://huggingface.co/blog/infini-attention), смогут ли они воспроизвести впечатляющие результаты из этой статьи. Первые эксперименты показали, что модель в принципе использует память, но не справляется с задачкой поиска спрятанного пароля в раннем сегменте. Кроме того, все этеншн-головы получились "миксованные", а не "специализированные". Разные трюки и подкрутки гиперпараметров улучшили ситуацию, но в целом авторы делают вывод, что при обучении длинноконтекстных LLM лучше пока придерживаться стандартных, проверенных техник. 

Именно так и поступила команда Qwen при [обучении Qwen2.5-1M](https://qwenlm.github.io/blog/qwen2.5-1m/). Простой, рабоче-крестьянский и надёжный способ улучшения работы моделей на задачах, требующих длинного контекста - дообучение ровно на таких задачах. Как же добыть такие данные? Квеновцы придумали [прикольный способ](https://qwenlm.github.io/blog/qwen-agent-2405/):

  * Поверх обычной 8k-модели создаём агента на основе продвинутого RAG, который может успешно отвечать на вопросы пользователей, требующие понимания длинного контекста.
  * Сажаем людей общаться с этим агентом и используем их диалоги в качестве обучающих данных.



Только дообучения, однако, недостаточно для достижения по-настоящему больших контекстов. Обучение на 256k токенов - уже очень недешёвое удовольствие, как же "раздвинуть" контекст до миллиона токенов?

Одна из важных причин деградации качества моделей на длинных контекстах - необученные позиционные эмбеддинги. Модель знает, что значит, что токены находятся на расстоянии 250k друг от друга, но никогда не видела токены, которые находятся в 700k токенах. Традиционный метод исправления ситуации - это [Position Interpolation](https://arxiv.org/abs/2306.15595). Идея проста - сжимаем все индексы таким образом, чтобы они ложились в исходный диапазон. Рекомендуется короткий файн-тюнинг для адаптации модели к интерполированным эмбеддингам.

![](/assets/images/6d2bddcc.png)

В Qwen2.5-1M используется [Dual Chunk Attention](https://arxiv.org/abs/2402.17463), которая вообще не требует никакого дообучения. Длинная последовательность делится на чанки размером меньше, чем максимальный контекст модели при обучении. Внутри чанка используется стандартный этеншн с обычным RoPE. Для остальных чанков (кроме соседнего) используется константная большая позиция (например, максимальный контекст модели), чтобы модель "понимала", что это какие-то дальние куски текста.

![](/assets/images/74919abf.png)

Ещё один важный компонент многих моделей с большим контекстом - [YaRN](https://arxiv.org/abs/2309.00071), он решает проблему ванильной интерполяции, которая одинаково рескейлит все измерения, и это сильно влияет на измерения с маленькой длиной волны. Подробнее об этом можно почитать [тут](https://blog.eleuther.ai/yarn/) и [тут](https://medium.com/@zaiinn440/linear-rope-vs-ntk-vs-yarn-vs-cope-d33587ddfd35).

Даже если модель успешно справляется с большим контекстом, но инференс требует нереальной кучи GPU, то в реальной жизни такую модель не поиспользуешь. Для оптимизации инференса в Qwen2.5-1M интегрированы идеи из [MInference](https://arxiv.org/abs/2407.02490). Например, если посмотреть на веса этеншна LLM, можно увидеть несколько интересных паттернов. Токены часто обращают внимание:

  * на начало последовательности и на соседние токены. Инструкция или описание задачи обычно располагаются в начале, а близкие токены часто взаимосвязаны. Это статический паттерн, он постоянен для любой последовательности.
  * на некоторые ключевые токены и на токены, расположенные на равных фиксированных интервалах от текущего ([здесь](https://github.com/microsoft/MInference/issues/21) и [здесь](https://colab.research.google.com/drive/1DbAEmmahKqIoXqdrxKWRzSToS8asN1EW?authuser=2#scrollTo=5MFIvkIjRGTp) можно почитать, почему при обучении образуется такой паттерн). Это динамический паттерн, поэтому для вычисления важных токенов используются последние n векторов запросов (Q) и ключей (K).
  * на определённые блоки из всей матрицы внимания. Это самый динамический паттерн.

![](/assets/images/e17551ce.png)

Эти паттерны позволяют значительно ускорить [prefill-стадию](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/) работы LLMи. Кстати, есть вот такой балдёжный [репозиторий](https://github.com/NVIDIA/kvpress) от NVIDIA, в котором собраны и реализованые разные способы компресии KV-кэша, что очень важно как раз для работы с большими контекстами.

Об остальных фишках можно почитать в их отличном [техническом репорте](https://arxiv.org/abs/2501.15383).

#### Минусы больших размеров

Ну так что же, RAG отправляется на свалку истории или нет? Кажется, нет:

  * В куче реальных бизнес-кейсов объём документов, с которыми надо работать, сильно превышает даже 1 миллион токенов. Даже в небольших компаниях все чаты, базы данных, код, документация легко переваливают за эту цифру. Это я ещё не учитываю документы из интернета.
  * Огромная стоимость инференса с большим контекстом. На одной RTX 4090 24GB максимальный контекст 14-миллиардной модели в квантизации Q6_K составляет порядка 48к токенов. У всех есть кластер из H100 под рукой?
  * Скорость инференса - вытащить даже с помощью какого-нибудь хитрого RAG на стероидах 10 релевантных документов и загнать их в LLM сильно быстрее, чем промалывать миллион токенов.
  * Реальный контекст моделей на реальных задачах всегда сильно-сильно меньше заявленного. Часто контекстные бенчмарки используют искусственные задачи типа поиска "иголки в стоге сена", а в недавней [статье](https://arxiv.org/abs/2502.05167) решили немного усложнить задачу. К примеру для иголки "Жека работает у БКЗ на Лиговском" вопрос будет звучать не "Кто из упомянутых людей работает у БКЗ на Лиговском?", а "Кто из упомянутых людей живёт в Санкт-Петербурге?". Модели нужно не просто найти фрагмент в длинном тексте, а ещё и сделать логический переход ("БКЗ на Лиговском, скорее всего, находится в Петербурге"). Результаты, скажем так, впечатляющие:

![](/assets/images/ef9272d7.png)

  * На практике тоже всегда заметно, что на контекстах даже в 50% от заявленного начинается явная деградация качества и забывание.

![](/assets/images/81e86c4e.jpg)

  


  * Методы расширения контекста иногда приводят к падению качества на обычных контекстах.
  * RAG лучше подходит для кейсов, где важно цитирование - ссылка на конкретный источник, который можно открыть и изучить.



Вывод простой и вечный - всегда думайте, а что за задачу я решаю, какой инструмент тут подойдёт больше? К примеру, для задачи саммеризации жирной взаимосвязанной кодовой базы большой контекст действительно важен. В нашей ИИ-вики используется и RAG (для поиска нужных для ответа на вопрос источников), и большой контекст (для саммеризации больших документов при индексации).
