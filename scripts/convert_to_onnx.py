#!/usr/bin/env python3
"""
–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è rubert-mini-frida –≤ ONNX —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –±—Ä–∞—É–∑–µ—Ä–µ
"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import onnx
import os
from pathlib import Path

def convert_rubert_to_onnx():
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç rubert-mini-frida –≤ ONNX —Ñ–æ—Ä–º–∞—Ç"""
    
    model_name = "sergeyzh/rubert-mini-frida"
    output_dir = Path("assets/onnx")
    output_dir.mkdir(exist_ok=True)
    
    print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_name}...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    model.eval()
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    sample_text = "search_query: —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"
    inputs = tokenizer(
        sample_text,
        max_length=512,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    
    print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ONNX...")
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ ONNX
    onnx_path = output_dir / "rubert-mini-frida.onnx"
    
    torch.onnx.export(
        model,
        (inputs["input_ids"], inputs["attention_mask"]),
        str(onnx_path),
        export_params=True,
        opset_version=14,
        do_constant_folding=True,
        input_names=["input_ids", "attention_mask"],
        output_names=["last_hidden_state"],
        dynamic_axes={
            "input_ids": {0: "batch_size", 1: "sequence"},
            "attention_mask": {0: "batch_size", 1: "sequence"},
            "last_hidden_state": {0: "batch_size", 1: "sequence"}
        }
    )
    
    print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {onnx_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
    vocab_path = output_dir / "vocab.txt"
    tokenizer_vocab = tokenizer.get_vocab()
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
    sorted_vocab = sorted(tokenizer_vocab.items(), key=lambda x: x[1])
    
    with open(vocab_path, 'w', encoding='utf-8') as f:
        for token, _ in sorted_vocab:
            f.write(f"{token}\n")
    
    print(f"‚úÖ Vocab —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {vocab_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    config_path = output_dir / "config.json"
    config = {
        "model_name": model_name,
        "max_length": 512,
        "embedding_dimension": 312,
        "vocab_size": len(tokenizer_vocab),
        "special_tokens": {
            "cls_token": tokenizer.cls_token,
            "sep_token": tokenizer.sep_token,
            "pad_token": tokenizer.pad_token,
            "unk_token": tokenizer.unk_token,
            "cls_token_id": tokenizer.cls_token_id,
            "sep_token_id": tokenizer.sep_token_id,
            "pad_token_id": tokenizer.pad_token_id,
            "unk_token_id": tokenizer.unk_token_id
        }
    }
    
    import json
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {config_path}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º ONNX –º–æ–¥–µ–ª—å
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º ONNX –º–æ–¥–µ–ª—å...")
    
    import onnxruntime as ort
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
    session = ort.InferenceSession(str(onnx_path))
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
    ort_inputs = {
        "input_ids": inputs["input_ids"].numpy(),
        "attention_mask": inputs["attention_mask"].numpy()
    }
    
    ort_outputs = session.run(None, ort_inputs)
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
    with torch.no_grad():
        torch_outputs = model(**inputs)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–∏–∑–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    diff = torch.abs(torch.tensor(ort_outputs[0]) - torch_outputs.last_hidden_state).max()
    print(f"üìä –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É PyTorch –∏ ONNX: {diff:.6f}")
    
    if diff < 1e-4:
        print("‚úÖ ONNX –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    else:
        print("‚ö†Ô∏è –ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é")
    
    print(f"\nüéâ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
    print(f"   - {onnx_path.name} ({onnx_path.stat().st_size / 1024 / 1024:.1f} MB)")
    print(f"   - {vocab_path.name} ({vocab_path.stat().st_size / 1024:.1f} KB)")
    print(f"   - {config_path.name}")

if __name__ == "__main__":
    convert_rubert_to_onnx()
